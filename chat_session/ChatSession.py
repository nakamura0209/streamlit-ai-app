# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒãƒ£ãƒƒãƒˆå…¥åŠ›ã‚’ä¼šè©±ã«è¿½åŠ ã™ã‚‹é–¢æ•°
from logging import Logger
import traceback
from typing import Any, Dict, List, Tuple
import openai
import streamlit as st
from chat_session.initialize_chat_page import initialize_sidebar, select_model
from data_source.langchain.lang_chain_chat_model_factory import ModelParameters
from data_source.openai_data_source import MODELS, Role
from logs.app_logger import set_logging
from logs.log_decorator import log_decorator

logger: Logger = set_logging("lower.sub")


class ChatSession:
    def __init__(self):
        # self.is_error = False
        if "messages" not in st.session_state:
            st.session_state.messages = [{"role": Role.SYSTEM.value, "content": ""}]

    @log_decorator(logger)
    def initialize_chat_page_element(self) -> Tuple[ModelParameters, str]:
        # ãƒšãƒ¼ã‚¸ã®åŸºæœ¬æ§‹æˆã‚’åˆæœŸåŒ–
        st.set_page_config(page_title="Stream-AI-Chat", page_icon="ğŸ¤–")
        st.header("Stream-AI-Chat")
        st.sidebar.title("Options")

        # ãƒ¦ãƒ¼ã‚¶ãŒè¨­å®šã—ãŸå€¤ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®š
        (
            model_version,
            max_tokens,
            temperature,
            top_p,
            frequency_penalty,
            presence_penalty,
        ) = initialize_sidebar()

        # ç”»é¢ä¸Šã§ãƒ¦ãƒ¼ã‚¶ãŒå¥½ããªã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§å¥½ããªãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã§ãã‚‹
        llm = select_model(
            model_version, max_tokens, temperature, top_p, frequency_penalty, presence_penalty
        )

        return llm, model_version

    # ä¼šè©±ã‚’è¡¨ç¤ºã™ã‚‹é–¢æ•°
    @log_decorator(logger)
    def display_conversations(self, messages: List[Dict[str, Any]], is_error: bool) -> None:
        """
        ä¼šè©±ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã‚‚å«ã¿ã¾ã™ã€‚

        Args:
            messages (List[Dict[str, Any]]): ä¼šè©±ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆã€‚
            is_error (bool): ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸã‹ã©ã†ã‹ã®ãƒ•ãƒ©ã‚°ã€‚
        """
        for message in messages:
            role, content = message["role"], message["content"]
            if role == "user" or role == "assistant":
                with st.chat_message(role):
                    st.markdown(content)
            elif role == "system":
                if is_error:
                    with st.chat_message(role):
                        st.markdown(content)

    @log_decorator(logger)
    def add_user_chat_message(self, user_input: str) -> None:
        """
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒãƒ£ãƒƒãƒˆå…¥åŠ›ã‚’ä¼šè©±ã«è¿½åŠ ã—ã¾ã™ã€‚

        Args:
            user_input (str): ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒãƒ£ãƒƒãƒˆå…¥åŠ›ã€‚
        """
        st.session_state.messages.append({"role": Role.USER.value, "content": user_input})
        st.chat_message(Role.USER.value).markdown(user_input)

    # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®ãƒãƒ£ãƒƒãƒˆå¿œç­”ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°
    @log_decorator(logger)
    def generate_assistant_chat_response(
        self, model_version: str, llm: ModelParameters
    ) -> Tuple[bool, str, str]:
        """
        OpenAIã®Chat APIã‚’ä½¿ç”¨ã—ã¦ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®ãƒãƒ£ãƒƒãƒˆå¿œç­”ã‚’ç”Ÿæˆã—ã¾ã™ã€‚

        Args:
            model_version (str): é¸æŠã•ã‚ŒãŸè¨€èªãƒ¢ãƒ‡ãƒ«ã®ã‚­ãƒ¼ã€‚
            temperature (float): ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã®ãŸã‚ã®temperatureãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‚
            llm(ModelParameters): ä¼šè©±ã‚’è¡Œã†éš›ã®GPTãƒ¢ãƒ‡ãƒ«ã¨ãã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            bool: ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯Trueã€ãã‚Œä»¥å¤–ã¯Falseã€‚
        """
        try:
            with st.chat_message(Role.ASSISTANT.value):
                message_placeholder = st.empty()
                assistant_chat = ""
                # ã“ã‚Œã¾ã§ã®ä¼šè©±å±¥æ­´ã‚‚ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã«é€ä¿¡ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ãŸã‚
                messages_with_history = [
                    {"role": m["role"], "content": m["content"]} for m in st.session_state.messages
                ]
                # OpenAIã®Chat APIã‚’å‘¼ã³å‡ºã—ã¦å¿œç­”ã‚’ç”Ÿæˆ
                for response in openai.ChatCompletion.create(
                    engine=MODELS[model_version]["config"]["deployment_name"],
                    messages=messages_with_history,
                    temperature=llm.temperature,
                    max_tokens=llm.max_tokens,
                    top_p=llm.top_p,
                    frequency_penalty=llm.frequency_penalty,
                    presence_penalty=llm.presence_penalty,
                    stream=True,
                    stop=None,
                ):
                    if response.choices:  # type: ignore
                        assistant_chat += response.choices[0].delta.get("content", "")  # type: ignore
                        message_placeholder.markdown(assistant_chat + "â–Œ")
                message_placeholder.markdown(assistant_chat)

            st.session_state.messages.append({"role": Role.ASSISTANT.value, "content": assistant_chat})

        except openai.error.RateLimitError as e:  # type: ignore
            logger.warn(traceback.format_exc())
            err_content_message = "The execution interval is too short. Wait a minute and try again."
            with st.chat_message(Role.SYSTEM.value):
                st.markdown(err_content_message)
            return True, "", ""

        except Exception as e:
            logger.warn(traceback.format_exc())
            err_content_message = "Unexpected error. Contact the administrator."
            with st.chat_message(Role.SYSTEM.value):
                st.markdown(err_content_message)
            return True, "", ""

        # ä¼šè©±å±¥æ­´ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’å–å¾—ã™ã‚‹ãŸã‚ã€æ–‡å­—åˆ—ã«å¤‰æ›
        converted_historys = [item["content"] for item in messages_with_history]
        converted_history = " ".join(converted_historys)

        return False, converted_history, assistant_chat
