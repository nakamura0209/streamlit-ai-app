# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from typing import Any, Dict, List, Union, Tuple
from dotenv import load_dotenv
import openai
import traceback
import streamlit as st

# ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨å½¹å‰²ã‚’å®šç¾©ã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from data_source.langchain.lang_chain_chat_model_factory import ModelParameters
from data_source.openai_data_source import MODELS, Role


# ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‚’åˆæœŸåŒ–ã—ã¦ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®šã™ã‚‹é–¢æ•°
def initialize_sidebar(model_key: str) -> Tuple[int, float, float, float, float]:
    # é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
    model_parameter = MODELS[model_key]["parameter"]
    # å„ç¨®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ã‚’ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«è¨­å®š
    max_tokens = st.sidebar.slider(
        "max_tokens: ",  # æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
        min_value=1,
        max_value=model_parameter["max_tokens"],
        value=2048,
        step=1,
    )
    temperature = st.sidebar.slider(
        "temperature: ",  # æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        min_value=0.0,
        max_value=2.0,
        value=0.0,
        step=0.1,
    )
    top_p = st.sidebar.slider(
        "top_p: ",  # ãƒˆãƒƒãƒ—Pã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        min_value=0.0,
        max_value=model_parameter["max_top_p"],
        value=0.0,
        step=0.1,
    )
    frequency_penalty = st.sidebar.slider(
        "frequency_penalty: ",  # é »åº¦ãƒšãƒŠãƒ«ãƒ†ã‚£
        min_value=0.0,
        max_value=model_parameter["max_frequency_penalty"],
        value=0.0,
        step=0.1,
    )
    presence_penalty = st.sidebar.slider(
        "presence_penalty: ",  # å­˜åœ¨ãƒšãƒŠãƒ«ãƒ†ã‚£
        min_value=0.0,
        max_value=model_parameter["max_presence_penalty"],
        value=0.0,
        step=0.1,
    )

    # è¨­å®šã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¿”ã™
    return max_tokens, temperature, top_p, frequency_penalty, presence_penalty


# ä¼šè©±ã‚’è¡¨ç¤ºã™ã‚‹é–¢æ•°
def display_conversations(messages: List[Dict[str, Any]], is_error: bool) -> None:
    """
    ä¼šè©±ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã‚‚å«ã¿ã¾ã™ã€‚

    Args:
        messages (List[Dict[str, Any]]): ä¼šè©±ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆã€‚
        is_error (bool): ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸã‹ã©ã†ã‹ã®ãƒ•ãƒ©ã‚°ã€‚
    """
    for message in messages:
        role, content = message["role"], message["content"]
        if role == "user" or role == "assistant":
            with st.chat_message(role):
                st.markdown(content)
        elif role == "system":
            if is_error:
                with st.chat_message(role):
                    st.markdown(content)


# ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®šã™ã‚‹é–¢æ•°
def select_model(
    model_key: str,
    max_tokens: int,
    temperature: float,
    top_p: float,
    frequency_penalty: float,
    presence_penalty: float,
) -> ModelParameters:
    """
    è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã€ãã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®šã—ã¾ã™ã€‚

    Args:
        model_key (str): é¸æŠã•ã‚ŒãŸè¨€èªãƒ¢ãƒ‡ãƒ«ã®ã‚­ãƒ¼ã€‚
        temperature (float): ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã®ãŸã‚ã®temperatureãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‚

    Returns:
        ModelParameters: é¸æŠã•ã‚ŒãŸè¨€èªãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‚
    """
    # é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®è¨­å®šã‚’å–å¾—
    model_config = MODELS[model_key]["config"]

    # OpenAI APIã®è¨­å®šã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã«ä¿å­˜
    st.session_state["openai_model"] = model_config["model_version"]
    openai.api_type, openai.api_base, openai.api_version, openai.api_key = (
        model_config["api_type"],
        model_config["base_url"],
        model_config["api_version"],
        model_config["api_key"],
    )
    print(openai.api_type)

    # é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®š
    llm = ModelParameters(
        max_tokens=max_tokens,
        temperature=temperature,
        top_p=top_p,
        frequency_penalty=frequency_penalty,
        presence_penalty=presence_penalty,
        deployment_name=model_config["deployment_name"],
    )

    # é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’æƒ…å ±ã¨ã—ã¦è¡¨ç¤º
    st.info(f"{model_key} is selected")

    # è¨­å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¿”ã™
    return llm


# ãƒãƒ£ãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã‚’åˆæœŸåŒ–ã™ã‚‹é–¢æ•°
def initialize_message_state() -> None:
    """
    ãƒãƒ£ãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã‚’åˆæœŸåŒ–ã—ã¾ã™ã€‚
    """
    # ã‚¯ãƒªã‚¢ãƒœã‚¿ãƒ³ã‚’ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«è¨­å®š
    clear_button = st.sidebar.button("Clear", key="clear")
    if clear_button:
        st.info("Conversation history has been deleted.")
    if clear_button or "messages" not in st.session_state:
        st.session_state.messages = []
        st.session_state.costs = []


# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒãƒ£ãƒƒãƒˆå…¥åŠ›ã‚’ä¼šè©±ã«è¿½åŠ ã™ã‚‹é–¢æ•°
def add_user_chat_message(user_input: str) -> None:
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒãƒ£ãƒƒãƒˆå…¥åŠ›ã‚’ä¼šè©±ã«è¿½åŠ ã—ã¾ã™ã€‚

    Args:
        user_input (str): ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒãƒ£ãƒƒãƒˆå…¥åŠ›ã€‚
    """
    st.session_state.messages.append({"role": Role.UESR.value, "content": user_input})
    st.chat_message(Role.UESR.value).markdown(user_input)


# ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®ãƒãƒ£ãƒƒãƒˆå¿œç­”ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°
def generate_assistant_chat_response(model_key: str, temperature: float, llm: ModelParameters) -> bool:
    """
    OpenAIã®Chat APIã‚’ä½¿ç”¨ã—ã¦ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®ãƒãƒ£ãƒƒãƒˆå¿œç­”ã‚’ç”Ÿæˆã—ã¾ã™ã€‚

    Args:
        model_key (str): é¸æŠã•ã‚ŒãŸè¨€èªãƒ¢ãƒ‡ãƒ«ã®ã‚­ãƒ¼ã€‚
        temperature (float): ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã®ãŸã‚ã®temperatureãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‚

    Returns:
        bool: ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯Trueã€ãã‚Œä»¥å¤–ã¯Falseã€‚
    """
    try:
        with st.chat_message(Role.ASSISTANT.value):
            message_placeholder = st.empty()
            full_response = ""
            # OpenAIã®Chat APIã‚’å‘¼ã³å‡ºã—ã¦å¿œç­”ã‚’ç”Ÿæˆ
            for response in openai.ChatCompletion.create(
                engine=MODELS[model_key]["config"]["deployment_name"],
                messages=[
                    {"role": m["role"], "content": m["content"]} for m in st.session_state.messages
                ],
                temperature=temperature,
                max_tokens=llm.max_tokens,
                top_p=llm.top_p,
                frequency_penalty=llm.frequency_penalty,
                presence_penalty=llm.presence_penalty,
                stream=True,
                stop=None,
            ):
                if response.choices:  # type: ignore
                    full_response += response.choices[0].delta.get("content", "")  # type: ignore
                    message_placeholder.markdown(full_response + "â–Œ")
            message_placeholder.markdown(full_response)

        st.session_state.messages.append({"role": Role.ASSISTANT.value, "content": full_response})

    except openai.error.RateLimitError as e:  # type: ignore
        print(e)
        err_content_message = "The execution interval is too short. Wait a minute and try again."
        with st.chat_message(Role.SYSTEM.value):
            st.markdown(err_content_message)
        return True

    except Exception as e:
        print(traceback.format_exc())
        err_content_message = "Unexpected error. Contact the administrator."
        with st.chat_message(Role.SYSTEM.value):
            st.markdown(err_content_message)
        return True

    return False


# ãƒ¡ã‚¤ãƒ³é–¢æ•°
def main():
    # ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€
    load_dotenv()
    is_error = False

    # åŸºæœ¬çš„ãªãƒšãƒ¼ã‚¸æ§‹é€ ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
    st.set_page_config(page_title="Stream-AI-Chat", page_icon="ğŸ¤–")
    st.header("Stream-AI-Chat")
    st.sidebar.title("Options")

    # è¨€èªãƒ¢ãƒ‡ãƒ«ã¨temperatureã‚’é¸æŠ
    model_key: Union[str, Any] = st.sidebar.radio("Select a model:", (MODELS.keys()))

    max_tokens, temperature, top_p, frequency_penalty, presence_penalty = initialize_sidebar(model_key)

    llm = select_model(model_key, max_tokens, temperature, top_p, frequency_penalty, presence_penalty)

    # ãƒãƒ£ãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã‚’åˆæœŸåŒ–
    initialize_message_state()

    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®åˆæœŸåŒ–
    if not st.session_state["messages"]:
        st.session_state.messages = [{"role": Role.SYSTEM.value, "content": ""}]

    # ä¼šè©±ã‚’è¡¨ç¤ºï¼ˆãƒãƒ£ãƒƒãƒˆå±¥æ­´å«ã‚€ï¼‰
    display_conversations(st.session_state.messages, is_error)

    # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚’å—ã‘ä»˜ã‘
    user_input = st.chat_input("Input your message...")
    if user_input:
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã‚’è¡¨ç¤º
        add_user_chat_message(user_input)
        # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®ãƒãƒ£ãƒƒãƒˆå¿œç­”ã‚’ç”Ÿæˆ
        is_error = generate_assistant_chat_response(model_key, temperature, llm)


# ãƒ¡ã‚¤ãƒ³é–¢æ•°ã‚’å®Ÿè¡Œ
if __name__ == "__main__":
    main()
