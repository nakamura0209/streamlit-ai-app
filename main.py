import os
from typing import Any, List, Union
from urllib import response
from altair import Stream
from dotenv import load_dotenv
from numpy import isin
import openai
import streamlit as st
from langchain.callbacks import StreamlitCallbackHandler
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import SystemMessage, HumanMessage, AIMessage

from data_source.langchain.lang_chain_chat_model_factory import (
    LangchainChatModelFactory,
    ModelParameters,
)
from data_source.openai_data_source import MODELS, Role


def create_converstations(
    messages: List[Union[HumanMessage, AIMessage, SystemMessage]], is_error: bool
) -> None:
    # ä¼šè©±ã®å±¥æ­´ã‚‚å«ã‚ã¦ã‚„ã‚Šå–ã‚Šã‚’æç”»
    for message in messages:
        if message.type == "ai":
            with st.chat_message(Role.ASSISTANT.value):
                st.markdown(message.content)
        elif message.type == "human":
            with st.chat_message(Role.UESR.value):
                st.markdown(message.content)
        else:
            if is_error:
                with st.chat_message(Role.SYSTEM.value):
                    st.markdown(message.content)


def generate_ai_messages(
    history_messages: List[Union[SystemMessage, Any]], llm: AzureChatOpenAI
) -> bool:
    try:
        with st.spinner("Generating ChatGPT answers..."):
            response = llm(history_messages)  # type: ignore
        st.session_state.messages.append(AIMessage(content=response.content))  # type: ignore

    except openai.error.RateLimitError as e:  # type: ignore
        err_content_message = "æ„Ÿè¦šãŒçŸ­ã™ãã¾ã™ã€‚ä¸€å®šæ™‚é–“çµŒéå¾Œã€å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"
        st.session_state.messages.append(SystemMessage(content=err_content_message))
        return True

    except Exception as e:
        print(e)
        err_content_message = "æƒ³å®šå¤–ã®ã‚¨ãƒ©ãƒ¼ã§ã™ã€‚ç®¡ç†è€…ã«å•ã„åˆã‚ã›ã¦ãã ã•ã„ã€‚"
        st.session_state.messages.append(SystemMessage(content=err_content_message))
        return True

    return False


def select_model(model: Union[str, Any], temperature: float) -> ModelParameters:
    # ãƒ¢ãƒ‡ãƒ«ã®åˆ‡ã‚Šæ›¿ãˆ
    print(model)
    st.session_state["openai_model"] = MODELS[model]["config"]["model_version"]

    # OpenAIåˆæœŸè¨­å®š
    openai.api_type = MODELS[model]["config"]["api_type"]
    print(openai.api_type)
    openai.api_base = MODELS[model]["config"]["base_url"]
    openai.api_version = MODELS[model]["config"]["api_version"]
    openai.api_key = MODELS[model]["config"]["api_key"]

    # ãƒãƒ£ãƒƒãƒˆç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä»˜ä¸ã—ãŸã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç”Ÿæˆ
    llm = ModelParameters(
        max_tokens=MODELS[model]["parameter"]["max_tokens"],
        temperature=temperature,
        top_p=MODELS[model]["parameter"]["top_p"],
        frequency_penalty=MODELS[model]["parameter"]["frequency_penalty"],
        presence_penalty=MODELS[model]["parameter"]["presence_penalty"],
        deployment_name=MODELS[model]["config"]["deployment_name"],
    )

    return llm


def init_message() -> None:
    clear_button = st.sidebar.button("Clear Conversation", key="clear")  # ä¼šè©±å±¥æ­´å‰Šé™¤ãƒœã‚¿ãƒ³
    if clear_button:
        st.info("Conversation history deleted.")
    if clear_button or "messages" not in st.session_state:
        st.session_state.messages = []
        st.session_state.costs = []


def main():
    # .envã‚’èª­ã¿å–ã‚‹
    load_dotenv()

    # ãƒšãƒ¼ã‚¸ã®åŸºæœ¬æ§‹æˆ
    ## ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«ã¨ãƒ˜ãƒƒãƒ€ã®è¨­å®š
    st.set_page_config(page_title="Stream-AI-Chat", page_icon="ğŸ¤–")
    st.header("Stream-AI-Chat")
    ## ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®è¨­å®š
    st.sidebar.title("Options")

    # AzureOpenAIChatã®ãƒ¢ãƒ‡ãƒ«ã¨temperatureã‚’é¸æŠã™ã‚‹
    model: Union[str, Any] = st.sidebar.radio("Choose a model: ", (MODELS.keys()))
    # ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ã®è¿½åŠ (min=0, max=2, default=0.0, stride=0.1)
    temperature = st.sidebar.slider("Temperature: ", min_value=0.0, max_value=2.0, value=0.0, step=0.1)

    llm = select_model(model, temperature)

    # ä¼šè©±å±¥æ­´ã®å‰Šé™¤(clearãƒœã‚¿ãƒ³ãŒæŠ¼ã•ã‚ŒãŸå ´åˆ)
    init_message()

    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®åˆæœŸåŒ–
    if not st.session_state["messages"]:
        st.session_state.messages = [{"role": Role.SYSTEM.value, "content": ""}]
    print(st.session_state)

    # å±¥æ­´ã‚‚å«ã‚ãŸä¼šè©±ã®ç”Ÿæˆ
    # create_converstations(st.session_state.messages, is_error)

    # ãƒ¦ãƒ¼ã‚¶å…¥åŠ›ã‚’ç›£è¦–
    user_input = st.chat_input("Input Your Message...")
    if user_input:
        st.session_state.messages.append({"role": Role.UESR.value, "content": user_input})  # type: ignore
        st.chat_message(Role.UESR.value).markdown(user_input)

        # TODO: ã“ã“ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§å®Ÿç¾ã—ãŸã„
        with st.chat_message(Role.ASSISTANT.value):
            message_placeholder = st.empty()  # ä¸€æ™‚çš„ãªãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’ä½œæˆ
            full_response = ""
            for response in openai.ChatCompletion.create(
                engine="openai-test-model",
                # model=st.session_state["openai_model"],
                messages=[
                    {"role": m["role"], "content": m["content"]} for m in st.session_state.messages
                ],
                temperature=0.7,
                max_tokens=800,
                top_p=0.95,
                frequency_penalty=0,
                presence_penalty=0,
                stream=True,
                stop=None,
            ):
                if response.choices:
                    full_response += response.choices[0].delta.get("content", "")
                    message_placeholder.markdown(full_response + "â–Œ")  # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®é€”ä¸­çµæœã‚’è¡¨ç¤º
            message_placeholder.markdown(full_response)  # æœ€çµ‚ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¡¨ç¤º

        st.session_state.messages.append({"role": Role.ASSISTANT.value, "content": full_response})
        print(st.session_state.messages)

        # ä¼šè©±å±¥æ­´ã‚’ã‚‚ã¨ã«å›ç­”ç”Ÿæˆé–‹å§‹
        # is_error = generate_ai_messages(st.session_state.messages, llm)


if __name__ == "__main__":
    main()
